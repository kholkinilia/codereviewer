{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## CodeReviewer applications and fine-tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edd71accdab5c236"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "The objective of this notebook is to look at predictions of the CodeReviewer model before and after fine-tuning for the Review generation task. The original paper of CodeReviewer can be found [here](https://arxiv.org/abs/2203.09095).\n",
    "\n",
    "I chose to look at the performance of the model on Kotlin programming language. It's interesting to look at, since Kotlin code was not present in the dataset. However, the model \"saw\" Java, which looks somewhat like Kotlin (maybe the with the help of beer). So, let's see if it's going to generalize to Kotlin or not. Of course, we don't expect it to generate something like \"rewrite it in Kotlin style\" before fine-tuning. \n",
    "\n",
    "What Kotlin repo to take? \n",
    "\n",
    "Let's take the [Kotlin Programming Language](https://github.com/JetBrains/kotlin) repo! It has plenty of PRs. Therefore, finding comments shouldn't be an issue. Although the Kotlin repo has its specifics. For example, comments like `\"Filed https://youtrack.jetbrains.com/issue/KT-44513\"`, which are impossible to predict without external knowledge. Nevertheless, I think all repos have their own specifics. So, it's a fine repo to look at.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38efed9081eb9c2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "But before we assemble a dataset with Kotlin diffs, we should understand, what the model actually wants from us as inputs. From the paper, it seems that we only need to provide diffs to the model. However, the paper also mentions adding tokens like `[ADD]`, `[DEL]` and `[KEEP]` for a different pre-training task (diff quality estimation).\n",
    "\n",
    "Strange, that for our task it doesn't want us to add those tokens. It should understand the code with tokens better since it was trained on them.\n",
    "\n",
    "Ok, let's try first without replacing `+` and `-` with `[ADD]` and `[DEL]`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79dac74baf41cdd1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import pipeline\n",
    "import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T00:59:23.321782Z",
     "start_time": "2023-09-17T00:59:21.938875Z"
    }
   },
   "id": "1a38feb1f6cdaa29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the dataset\n",
    "\n",
    "It's not mandatory, but if you want you can download the [official dataset](https://zenodo.org/record/6900648) (code provided in the following cell) and look at the predictions on different examples other than hardcoded. The examples are taken from the test part of the dataset.\n",
    "\n",
    "To install the dataset, change `download_cg_dataset` variable to `True`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61fda7ff7f626fe0"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "download_cg_dataset = False\n",
    "\n",
    "DATASET_DIR = \"./dataset\"\n",
    "CG_DATASET_DIR = os.path.join(DATASET_DIR, \"Comment_Generation\")\n",
    "\n",
    "if not os.path.exists(DATASET_DIR):\n",
    "    ! mkdir./dataset\n",
    "\n",
    "if download_cg_dataset and not os.path.exists(CG_DATASET_DIR):\n",
    "    !./scripts/cg_dataset.sh"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:02:00.922530Z",
     "start_time": "2023-09-16T22:02:00.918543Z"
    }
   },
   "id": "1f3920c2b839ebe2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's look at predictions of raw diffs!\n",
    "\n",
    "Let's just take a couple of samples from the test partition of the official dataset and look at the predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17a65076899db9bf"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_sample_diffs(with_target=False):\n",
    "    # Get samples from the dataset\n",
    "    with open(\"./dataset/comment_generation_smaple/sample.jsonl\", \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        diffs = [json.loads(line)['patch'] for line in lines]\n",
    "        targets = [json.loads(line)['msg'] for line in lines]\n",
    "    \n",
    "    if with_target:\n",
    "        return diffs, targets\n",
    "    return diffs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:02:02.168142Z",
     "start_time": "2023-09-16T22:02:02.163264Z"
    }
   },
   "id": "a896ca03eee37f1f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '<msg>Please remove this extra line'}\n",
      "{'generated_text': '<msg>This is not needed.'}\n",
      "{'generated_text': '<msg>I think this line is not needed'}\n",
      "{'generated_text': '<msg>This is a bit of a nitpick, but I think it would be better to'}\n",
      "{'generated_text': '<msg>This is a bit of a nitpick, but I think it would be better to'}\n",
      "{'generated_text': '<msg>I think this is a bit too much.'}\n",
      "{'generated_text': '<msg>This file is not part of the PR.'}\n",
      "{'generated_text': '<msg>please remove the extra line'}\n",
      "{'generated_text': '<msg>Please remove this extra line.'}\n",
      "{'generated_text': '<msg>This is a bit of a nitpick, but I think it would be better to'}\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text2text-generation\", \"microsoft/codereviewer\")\n",
    "\n",
    "diffs = get_sample_diffs()\n",
    "\n",
    "result = pipe(diffs)\n",
    "\n",
    "for c, r in zip(diffs, result):\n",
    "    print(r)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:18:06.414928Z",
     "start_time": "2023-09-16T22:18:01.827621Z"
    }
   },
   "id": "9e5792a368cf1e96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Doesn't look very specific :( \n",
    "\n",
    "Seems like it just generates responses from the pool of most like responses. Maybe we should preprocess the data as they said in the paper!\n",
    "\n",
    "Now, in the paper they didn't specify how to preprocess it. So, I went to their [official repo](https://github.com/microsoft/CodeBERT/tree/master/CodeReviewer) and looked at the script they propose to do predictions with: [run_infer_msg.py](https://github.com/microsoft/CodeBERT/blob/master/CodeReviewer/code/run_infer_msg.py).\n",
    "\n",
    "They use the following functions to preprocess the data. Let's use it as well!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3001eae44e6dfe63"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def add_special_tokens(diff_hunk: str):\n",
    "    diff_lines = diff_hunk.split(\"\\n\")[1:]        # remove start @@\n",
    "    diff_lines = [line for line in diff_lines if len(line.strip()) > 0]\n",
    "    map_dic = {\"-\": 0, \"+\": 1, \" \": 2}\n",
    "    def f(s):\n",
    "        if s in map_dic:\n",
    "            return map_dic[s]\n",
    "        else:\n",
    "            return 2\n",
    "    labels = [f(line[0]) for line in diff_lines]\n",
    "    diff_lines = [line[1:].strip() for line in diff_lines]\n",
    "    input_str = \"\"\n",
    "    for label, line in zip(labels, diff_lines):\n",
    "        if label == 1:\n",
    "            input_str += \"<add>\" + line\n",
    "        elif label == 0:\n",
    "            input_str += \"<del>\" + line\n",
    "        else:\n",
    "            input_str += \"<keep>\" + line\n",
    "    return input_str"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:18:10.857266Z",
     "start_time": "2023-09-16T22:18:10.855616Z"
    }
   },
   "id": "69dec8e20700a55a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function just replaces `-` and `+` with `<add>` and `<del>` tokens. It adds a `<keep>` token, strips the lines and removes the first line as well. \n",
    "\n",
    "Here is an example of what it does with a diff hunk."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23abcbdfa6c41324"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= INITIAL DIFF =============\n",
      "\n",
      "@@ -53,7 +53,7 @@ public class ProtocGapicPluginGeneratorTest {\n",
      "                 model.getFiles().stream().map(ProtoFile::getProto).collect(Collectors.toList()))\n",
      "             // Only the file to generate a client for (don't generate dependencies)\n",
      "             .addFileToGenerate(\"multiple_services.proto\")\n",
      "-            .setParameter(\"language=java\")\n",
      "+            .setParameter(\"language=java,transport=grpc\")\n",
      "             .build();\n",
      " \n",
      "     CodeGeneratorResponse response = ProtocGeneratorMain.generate(codeGeneratorRequest);\n",
      "\n",
      "============= PREPROCESSED DIFF =============\n",
      "\n",
      "<keep>model.getFiles().stream().map(ProtoFile::getProto).collect(Collectors.toList()))<keep>// Only the file to generate a client for (don't generate dependencies)<keep>.addFileToGenerate(\"multiple_services.proto\")<del>.setParameter(\"language=java\")<add>.setParameter(\"language=java,transport=grpc\")<keep>.build();<keep>CodeGeneratorResponse response = ProtocGeneratorMain.generate(codeGeneratorRequest);\n"
     ]
    }
   ],
   "source": [
    "diff = get_sample_diffs()[0]\n",
    "\n",
    "print(\"============= INITIAL DIFF =============\\n\")\n",
    "print(f\"{diff}\\n\")\n",
    "print(\"============= PREPROCESSED DIFF =============\\n\")\n",
    "print(add_special_tokens(diff))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:18:16.324862Z",
     "start_time": "2023-09-16T22:18:16.316040Z"
    }
   },
   "id": "cb44aa036727fc8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, let's see if the model will pay more attention to the code after preprocessing. (It should!)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81a6b5deb92e7478"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': '<msg>I think we should add a `.setParameter(\"language=java,transport=grpc\")` here.'}\n",
      "{'generated_text': '<msg>I think we should set this only if the index is configured.'}\n",
      "{'generated_text': \"<msg>I think we should use `window.analytics.load(<%= ENV['SEGMENT_KEY']%>);`\"}\n",
      "{'generated_text': \"<msg>I think this should be `isNaN(file.data.size) ? '' : html`\"}\n",
      "{'generated_text': \"<msg>I think we should use `getNavigateURL` here instead of `getAdminURL` because it's not the same as `getAdminURL` in the `getNavigateURL` function.\"}\n",
      "{'generated_text': '<msg>I think this should be `const AbsMat& im`'}\n",
      "{'generated_text': '<msg>nit: import order'}\n",
      "{'generated_text': '<msg>I think we should use `snprintf_s` here'}\n",
      "{'generated_text': \"<msg>I think this should be `getattr(api.getForegroundObject(), '_lastDetectedKeyboardLayoutChange', 0)`\"}\n",
      "{'generated_text': \"<msg>This is a bit weird, but I guess it's fine.\"}\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text2text-generation\", \"microsoft/codereviewer\", max_length=200)\n",
    "\n",
    "diffs = list(map(add_special_tokens, get_sample_diffs()))\n",
    "\n",
    "result = pipe(diffs)\n",
    "\n",
    "for c, r in zip(diffs, result):\n",
    "    print(r)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:18:25.543042Z",
     "start_time": "2023-09-16T22:18:19.949758Z"
    }
   },
   "id": "b9eaaf90a12a09c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, at least it looks more specific. Let's look, how the generated comments compare with the target ones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2bfc8e3afd1ce6a"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================\n",
      "Generated: I think we should add a `.setParameter(\"language=java,transport=grpc\")` here.\n",
      "Target: can we also test for `transport=rest`?'\n",
      "==========================================================================================================\n",
      "Generated: I think we should set this only if the index is configured.\n",
      "Target: If record_batch_size is not set in config.ini, this code will trigger a notice about an undefined value. I would suggest either wrapping the setPageSize() call in an `if (!empty(...)) {` check, or else providing a default value in the set call (i.e. `$config->Index->record_batch_size ?? 100`).'\n",
      "==========================================================================================================\n",
      "Generated: I think we should use `window.analytics.load(<%= ENV['SEGMENT_KEY']%>);`\n",
      "Target: I didn't realize we were hardcoding this, thanks for moving it to an env value.'\n",
      "==========================================================================================================\n",
      "Generated: I think this should be `isNaN(file.data.size) ? '' : html`\n",
      "Target: We are trying to support IE 10-11, so we'll need a polyfill for this one, I think.'\n",
      "==========================================================================================================\n",
      "Generated: I think we should use `getNavigateURL` here instead of `getAdminURL` because it's not the same as `getAdminURL` in the `getNavigateURL` function.\n",
      "Target: It looks like there's a new `isNavigatingTo( url )` selector for this very purpose so let's use this here instead. This way we just need to use the one selector rather than two. Let's assign that to a similar-named variable here (e.g. `isNavigatingToPostResetURL`) rather than the prop it's used with.'\n",
      "==========================================================================================================\n",
      "Generated: I think this should be `const AbsMat& im`\n",
      "Target: I think im2col should only accommodate CPUMat.'\n",
      "==========================================================================================================\n",
      "Generated: nit: import order\n",
      "Target: alpha sort the imports'\n",
      "==========================================================================================================\n",
      "Generated: I think we should use `snprintf_s` here\n",
      "Target: Can you explain why is this necessary? Is `snprintf()` with four integer arguments unsafe?'\n",
      "==========================================================================================================\n",
      "Generated: I think this should be `getattr(api.getForegroundObject(), '_lastDetectedKeyboardLayoutChange', 0)`\n",
      "Target: How likely would it be that the keyboard layout for the NVDA main thread differs from the keyboard layout of the currently focused app?'\n",
      "==========================================================================================================\n",
      "Generated: This is a bit weird, but I guess it's fine.\n",
      "Target: Would you mind removing the default parameter above? (I doubt I'll ever use defaults again since you still have to protect against callers explicitly passing `null` or `undefined`)'\n"
     ]
    }
   ],
   "source": [
    "_, target = get_sample_diffs(with_target=True)\n",
    "\n",
    "for generated, trg in zip(result, target):\n",
    "    print(\"==========================================================================================================\")\n",
    "    print(f\"Generated: {generated['generated_text'][5:]}\\nTarget: {trg}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:09:22.830864Z",
     "start_time": "2023-09-16T21:09:22.827061Z"
    }
   },
   "id": "5de93b746f3e3cde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice, I see at least one exactly matching meaning:\n",
    "\n",
    "```\n",
    "Generated: nit: import order\n",
    "Target: alpha sort the imports'\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "380340e1ee422fd4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's look as well to the comments with the code for the sake of completeness."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55da38a80876cc67"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -53,7 +53,7 @@ public class ProtocGapicPluginGeneratorTest {\n",
      "                 model.getFiles().stream().map(ProtoFile::getProto).collect(Collectors.toList()))\n",
      "             // Only the file to generate a client for (don't generate dependencies)\n",
      "             .addFileToGenerate(\"multiple_services.proto\")\n",
      "-            .setParameter(\"language=java\")\n",
      "+            .setParameter(\"language=java,transport=grpc\")\n",
      "             .build();\n",
      " \n",
      "     CodeGeneratorResponse response = ProtocGeneratorMain.generate(codeGeneratorRequest);\n",
      "\n",
      "=== Generated comment:\n",
      "I think we should add a `.setParameter(\"language=java,transport=grpc\")` here.\n",
      "\n",
      "=== Target:\n",
      "can we also test for `transport=rest`?\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -182,7 +182,9 @@ abstract class AbstractSolrBackendFactory implements FactoryInterface\n",
      "      */\n",
      "     protected function createBackend(Connector $connector)\n",
      "     {\n",
      "+        $config = $this->config->get($this->mainConfig);\n",
      "         $backend = new $this->backendClass($connector);\n",
      "+        $backend->setPageSize($config->Index->record_batch_size);\n",
      "         $backend->setQueryBuilder($this->createQueryBuilder());\n",
      "         $backend->setSimilarBuilder($this->createSimilarBuilder());\n",
      "         if ($this->logger) {\n",
      "\n",
      "=== Generated comment:\n",
      "I think we should set this only if the index is configured.\n",
      "\n",
      "=== Target:\n",
      "If record_batch_size is not set in config.ini, this code will trigger a notice about an undefined value. I would suggest either wrapping the setPageSize() call in an `if (!empty(...)) {` check, or else providing a default value in the set call (i.e. `$config->Index->record_batch_size ?? 100`).\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -1,6 +1,6 @@\n",
      " <script type=\"text/javascript\">\n",
      "   window.analytics||(window.analytics=[]),window.analytics.methods=[\"identify\",\"track\",\"trackLink\",\"trackForm\",\"trackClick\",\"trackSubmit\",\"page\",\"pageview\",\"ab\",\"alias\",\"ready\",\"group\",\"on\",\"once\",\"off\"],window.analytics.factory=function(t){return function(){var a=Array.prototype.slice.call(arguments);return a.unshift(t),window.analytics.push(a),window.analytics}};for(var i=0;i<window.analytics.methods.length;i++){var method=window.analytics.methods[i];window.analytics[method]=window.analytics.factory(method)}window.analytics.load=function(t){var a=document.createElement(\"script\");a.type=\"text/javascript\",a.async=!0,a.src=(\"https:\"===document.location.protocol?\"https://\":\"http://\")+\"d2dq2ahtl5zl1z.cloudfront.net/analytics.js/v1/\"+t+\"/analytics.min.js\";var n=document.getElementsByTagName(\"script\")[0];n.parentNode.insertBefore(a,n)},window.analytics.SNIPPET_VERSION=\"2.0.8\",\n",
      "-  window.analytics.load(\"2nexpdgku3\");\n",
      "+  window.analytics.load(<%= ENV['SEGMENT_KEY']%>);\n",
      "   window.analytics.page();\n",
      " </script>\n",
      " \n",
      "\n",
      "=== Generated comment:\n",
      "I think we should use `window.analytics.load(<%= ENV['SEGMENT_KEY']%>);`\n",
      "\n",
      "=== Target:\n",
      "I didn't realize we were hardcoding this, thanks for moving it to an env value.\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -99,7 +99,7 @@ module.exports = function fileItem (props) {\n",
      "         }\n",
      "       </h4>\n",
      "       <div class=\"UppyDashboardItem-status\">\n",
      "-        ${file.data.size && html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`}\n",
      "+        ${isNaN(file.data.size) ? '' : html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`}\n",
      "         ${file.source && html`<div class=\"UppyDashboardItem-sourceIcon\">\n",
      "             ${acquirers.map(acquirer => {\n",
      "               if (acquirer.id === file.source) return html`<span title=\"${props.i18n('fileSource')}: ${acquirer.name}\">${acquirer.icon()}</span>`\n",
      "\n",
      "=== Generated comment:\n",
      "I think this should be `isNaN(file.data.size) ? '' : html`\n",
      "\n",
      "=== Target:\n",
      "We are trying to support IE 10-11, so we'll need a polyfill for this one, I think.\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -38,6 +38,9 @@ const { useSelect, useDispatch } = Data;\n",
      " function ResetButton( { children } ) {\n",
      " \tconst postResetURL = useSelect( ( select ) => select( CORE_SITE ).getAdminURL( 'googlesitekit-splash', { notification: 'reset_success' } ) );\n",
      " \n",
      "+\tconst isNavigating = useSelect( ( select ) => select( CORE_LOCATION ).isNavigating() );\n",
      "+\tconst navigatingURL = useSelect( ( select ) => select( CORE_LOCATION ).getNavigateURL() );\n",
      "+\n",
      " \tconst [ dialogActive, setDialogActive ] = useState( false );\n",
      " \n",
      " \tuseEffect( () => {\n",
      "\n",
      "=== Generated comment:\n",
      "I think we should use `getNavigateURL` here instead of `getAdminURL` because it's not the same as `getAdminURL` in the `getNavigateURL` function.\n",
      "\n",
      "=== Target:\n",
      "It looks like there's a new `isNavigatingTo( url )` selector for this very purpose so let's use this here instead. This way we just need to use the one selector rather than two. Let's assign that to a similar-named variable here (e.g. `isNavigatingToPostResetURL`) rather than the prop it's used with.\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -29,8 +29,8 @@\n",
      " \n",
      " namespace lbann {\n",
      " \n",
      "-void im2col(const Mat& im,\n",
      "-            Mat& col,\n",
      "+void im2col(const AbsMat& im,\n",
      "+            AbsMat& col,\n",
      "             const int num_channels,\n",
      "             const int im_num_dims,\n",
      "             const int * im_dims,\n",
      "\n",
      "=== Generated comment:\n",
      "I think this should be `const AbsMat& im`\n",
      "\n",
      "=== Target:\n",
      "I think im2col should only accommodate CPUMat.\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -24,6 +24,8 @@ from google.cloud.forseti.notifier.notifiers import cscc_notifier\n",
      " from google.cloud.forseti.notifier.notifiers.inventory_summary import InventorySummary\n",
      " from google.cloud.forseti.services.inventory.storage import DataAccess\n",
      " from google.cloud.forseti.services.scanner import dao as scanner_dao\n",
      "+from google.cloud.forseti.common.util.email.email_factory import EmailFactory\n",
      "+from google.cloud.forseti.notifier.notifiers import email_violations\n",
      " # pylint: enable=line-too-long\n",
      " \n",
      " \n",
      "\n",
      "=== Generated comment:\n",
      "nit: import order\n",
      "\n",
      "=== Target:\n",
      "alpha sort the imports\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -1067,7 +1067,7 @@ fpga_result mmio_error(struct RASCommandLine *rasCmdLine)\n",
      " \tif ( rasCmdLine->function >0 )\n",
      " \t\tfunction = rasCmdLine->bus;\n",
      " \n",
      "-\tsnprintf(sysfs_path, sizeof(sysfs_path),\n",
      "+\tsnprintf_s_iiii(sysfs_path, sizeof(sysfs_path),\n",
      " \t\t\tDEVICEID_PATH,0,bus,device,function);\n",
      " \n",
      " \tresult = sysfs_read_u64(sysfs_path, &value);\n",
      "\n",
      "=== Generated comment:\n",
      "I think we should use `snprintf_s` here\n",
      "\n",
      "=== Target:\n",
      "Can you explain why is this necessary? Is `snprintf()` with four integer arguments unsafe?\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -219,7 +219,19 @@ def internal_keyDownEvent(vkCode,scanCode,extended,injected):\n",
      " \t\t\tfor k in range(256):\n",
      " \t\t\t\tkeyStates[k]=ctypes.windll.user32.GetKeyState(k)\n",
      " \t\t\tcharBuf=ctypes.create_unicode_buffer(5)\n",
      "+\t\t\t# First try getting the keyboard layout from the thread with the focus (input thread)\n",
      " \t\t\thkl=ctypes.windll.user32.GetKeyboardLayout(focus.windowThreadID)\n",
      "+\t\t\tif not hkl:\n",
      "+\t\t\t\tlog.debug(\"Failed to fetch keyboard layout from focus, trying layout from last detected change\")\n",
      "+\t\t\t\t# Some threads, such as for Windows consoles\n",
      "+\t\t\t\t# Do not allow getKeyboardLayout to work.\n",
      "+\t\t\t\t# Therefore, use the cached keyboard layout from the last inputLangChange detected by NVDA\n",
      "+\t\t\t\t# on the foreground object.\n",
      "+\t\t\t\thkl = getattr(api.getForegroundObject(), '_lastDetectedKeyboardLayoutChange', 0)\n",
      "+\t\t\t\tif not hkl:\n",
      "+\t\t\t\t\tlog.debug(\"No layout cached, falling back to layout of NVDA main thread\")\n",
      "+\t\t\t\t\t# As a last resort, use the keyboard layout of NVDA's main thread.\n",
      "+\t\t\t\t\thkl = ctypes.windll.user32.GetKeyboardLayout(core.mainThreadId)\n",
      " \t\t\t# In previous Windows builds, calling ToUnicodeEx would destroy keyboard buffer state and therefore cause the app to not produce the right WM_CHAR message.\n",
      " \t\t\t# However, ToUnicodeEx now can take a new flag of 0x4, which stops it from destroying keyboard state, thus allowing us to safely call it here.\n",
      " \t\t\tres=ctypes.windll.user32.ToUnicodeEx(vkCode,scanCode,keyStates,charBuf,len(charBuf),0x4,hkl)\n",
      "\n",
      "=== Generated comment:\n",
      "I think this should be `getattr(api.getForegroundObject(), '_lastDetectedKeyboardLayoutChange', 0)`\n",
      "\n",
      "=== Target:\n",
      "How likely would it be that the keyboard layout for the NVDA main thread differs from the keyboard layout of the currently focused app?\n",
      "==========================================================================================================\n",
      "=== Diff:\n",
      "@@ -198,6 +198,10 @@ class Driver extends webdriver.WebDriver {\n",
      "    * @return {!Driver} A new driver instance.\n",
      "    */\n",
      "   static createSession(options, service = getDefaultService()) {\n",
      "+    if (!service) {\n",
      "+      service = getDefaultService();\n",
      "+    }\n",
      "+    \n",
      "     let client = service.start().then(url => new http.HttpClient(url));\n",
      "     let executor = new http.Executor(client);\n",
      " \n",
      "\n",
      "=== Generated comment:\n",
      "This is a bit weird, but I guess it's fine.\n",
      "\n",
      "=== Target:\n",
      "Would you mind removing the default parameter above? (I doubt I'll ever use defaults again since you still have to protect against callers explicitly passing `null` or `undefined`)\n"
     ]
    }
   ],
   "source": [
    "for generated, code, trg in zip(result, get_sample_diffs(), target):\n",
    "    print(\"==========================================================================================================\")\n",
    "    print(f\"=== Diff:\\n{code}\\n\\n=== Generated comment:\\n{generated['generated_text'][5:]}\\n\\n=== Target:\\n{trg}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:26:30.749715Z",
     "start_time": "2023-09-16T21:26:30.746437Z"
    }
   },
   "id": "a18ad7e566986739"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a lot of cases it just says: I think we should add [something that is being added already in the diff] :)\n",
    "\n",
    "However, most of the other comments at least make sense. And a couple of them are extremely close to the ground truth. \n",
    "For example, the \"import order\" and the \"I think we should set this only if the index is configured\" ones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba11e5bb0c50f59a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also look at what the model generates on the examples from the Figure 5 the [paper](https://arxiv.org/pdf/2203.09095.pdf) (I found them in the test partition of the original dataset as well)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b91955db9b8098c0"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<msg>I think this should return an empty list instead of null.\n",
      "<msg>I think this should be `LOG.finest(\"Mapping {} to docker image {} times\", caps, name, maxContainerCount);`\n"
     ]
    }
   ],
   "source": [
    "code = [\n",
    "\"\"\"@@ -388,4 +388,10 @@ public class MockExecutorLoader implements ExecutorLoader {\n",
    "   public void unassignExecutor(int executionId) throws ExecutorManagerException {\n",
    "     executionExecutorMapping.remove(executionId);\n",
    "   }\n",
    "+\n",
    "+  @Override\n",
    "+  public List<ExecutableFlow> fetchRecentlyFinishedFlows(long lifeTimeMs)\n",
    "+      throws ExecutorManagerException {\n",
    "+    return null;\n",
    "+  }\n",
    " }\n",
    "\"\"\",\n",
    "\"\"\"@@ -124,7 +124,7 @@ public class DockerOptions {\n",
    "       for (int i = 0; i < maxContainerCount; i++) {\n",
    "         node.add(caps, new DockerSessionFactory(clientFactory, docker, image, caps));\n",
    "       }\n",
    "-      LOG.info(String.format(\n",
    "+      LOG.finest(String.format(\n",
    "           \"Mapping %s to docker image %s %d times\",\n",
    "           caps,\n",
    "           name,\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "code = list(map(add_special_tokens, code))\n",
    "\n",
    "paper_diffs = pipe(code)\n",
    "\n",
    "for gen_comment in paper_diffs:\n",
    "    print(gen_comment['generated_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T21:25:10.542862Z",
     "start_time": "2023-09-16T21:25:09.659058Z"
    }
   },
   "id": "bf1db7e40a1f6c1b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, the first response matches with the ground truth, but the second is a classical \"I think this should be [already added stuff]\".\n",
    "\n",
    "Their dataset might have a lot of \"I think this should be the stuff I added\" comments from the authors of the code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3e63c80e301b162"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The moment of Kotlin!\n",
    "\n",
    "Now that we know what the model wants from us, let's gather our own Kotlin mini-dataset.\n",
    "\n",
    "A reader doesn't have to execute scripts that gather the dataset, because the mini-dataset is committed to the repo (`dataset/github` dir).\n",
    "\n",
    "I wrote two scripts, which are stored in the `scripts` folder: `github_dataset.py` and `github_prep.py`. They download all the PR comments since 2020 from the `jetbrains/kotlin` repo.\n",
    "\n",
    "Basically, I filtered the comments just like they did in the paper: kept only the ones that don't have replies. I also additionally filtered out the diffs that were longer than 1000 characters to exclude new added long files from coverage. Such files are useless to process, because they require several comments, but not one. In the end it resulted in **763 comments**. They are contained in `dataset/github/jetbrains_kotlin_pure.jsonl` file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8efbe9f10292877a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's generates the comments for the dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bea09ef534c8e9d2"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [04:00<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./dataset/github/jetbrains_kotlin_pure.jsonl\", \"r\") as file:\n",
    "    items = [json.loads(line) for line in file.readlines()]\n",
    "    \n",
    "query = [item['patch'] for item in items]\n",
    "query = list(map(add_special_tokens, query))\n",
    "\n",
    "outputs = []\n",
    "\n",
    "batch_size = 5\n",
    "for i in tqdm(range(0, len(query), batch_size)):\n",
    "    batch = query[i:i + batch_size]\n",
    "    generated = pipe(batch)\n",
    "    outputs += generated\n",
    "    \n",
    "    \n",
    "predictions_and_targets = []\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    pred_and_target = {\n",
    "        \"pred\": output['generated_text'],\n",
    "        \"target\": items[i]['msg'],\n",
    "        \"id\": i,\n",
    "    }\n",
    "    predictions_and_targets.append(pred_and_target)\n",
    "    \n",
    "with open(\"./dataset/github/jetbrains_kotlin_preds.jsonl\", \"w\") as file:\n",
    "    file.write(\"\\n\".join([json.dumps(pred_and_target) for pred_and_target in predictions_and_targets]))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T22:55:46.266010Z",
     "start_time": "2023-09-16T22:51:46.036005Z"
    }
   },
   "id": "6298e8e7624e79ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at some predictions!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef8c3abc2004e765"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================\n",
      "Generated:\n",
      "This is not correct. The `targetCallable` is a `FunctionImportedFromObject` or a `Callable`.\n",
      "\n",
      "Target:\n",
      "Should we use `ImportedFromObjectCallableDescriptor<*>` instead?\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "I think we can remove this import.\n",
      "\n",
      "Target:\n",
      "Minor: unused import. I'll remove it before pushing.\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "I think this is a bug.\n",
      "\n",
      "Target:\n",
      "I somehow missed it. Thanks!\r\n",
      "Minor: there is `SUSPEND_CALL_RESULT_NAME`, which, in fact, I should've used.\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "This is the same as `this.functionStack.lastOrNull()?.dispatchReceiverParameter`\n",
      "\n",
      "Target:\n",
      "I will change it to `thisReceiverExpression.convertWithOffsets` usage\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "This is a bit of a hack, but I'm not sure how to do it better.\n",
      "\n",
      "Target:\n",
      "correct one is: \r\n",
      "```\r\n",
      "   val b = number.toByte()\r\n",
      "   val d = number.toDouble()\r\n",
      "   val f = number.toFloat()\r\n",
      "   val i = number.toInt()\r\n",
      "   val l = number.toLong()\r\n",
      "   val s = number.toShort()\r\n",
      "```\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "\n",
      "\n",
      "Target:\n",
      "This test should not have passed. It is checking that no delegate field is created for interface delegation. It is doing so based on the name of the field, and therefore it now correctly fails because we do not have that optimization.\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "\n",
      "\n",
      "Target:\n",
      "There are also test `namedArgumentsBefore.kt` which might fail now, because by default `MixedNamedArgumentsInTheirOwnPosition` will be enabled in kotlin 1.4\r\n",
      "\r\n",
      "Can you please explicitly specify there that `MixedNamedArgumentsInTheirOwnPosition` is disabled in this test (by adding `// COMPILER_ARGUMENTS: -XXLanguage:-MixedNamedArgumentsInTheirOwnPosition`), and add additional test with `MixedNamedArgumentsInTheirOwnPosition` enabled (in which the intention should now work for this case)?\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "I think this is not needed.\n",
      "\n",
      "Target:\n",
      "It seems like there are missing test when `MixedNamedArgumentsInTheirOwnPosition` is enabled, but the arguments are not in their own positions, and the intention should be disabled\r\n",
      "\r\n",
      "With your code, this case will look like `foo(1, <caret>name3 = 3, name2 = 2)`\r\n",
      "\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "This is a bit of a hack, but I'm not sure how to make it work.\n",
      "\n",
      "Target:\n",
      "When `MixedNamedArgumentsInTheirOwnPosition` is enabled, you cannot skip this check. Actually you have to check that all of the previous arguments are not named, or are named but in their own positions\r\n",
      "\r\n",
      "You can take a look at `NamedArgumentCompletion::isOnlyNamedArgumentExpected` to see how this check looks exactly\r\n",
      "\r\n",
      "To check that your implementation is correct, try this test: \r\n",
      "\r\n",
      "```kt\r\n",
      "// COMPILER_ARGUMENTS: -XXLanguage:+MixedNamedArgumentsInTheirOwnPosition\r\n",
      "// IS_APPLICABLE: false\r\n",
      "\r\n",
      "fun foo(name1: Int, name2: Int, name3: Int) {}\r\n",
      "\r\n",
      "fun usage() {\r\n",
      "    foo(name2 = 2, name1 = 1, <caret>name3 = 3)\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "The intention should not be applicable, because first two arguments are not in their own positions, therefore you cannot remove name from the third argument\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "This is a bit of a hack, but I'm not sure how to do it better.\n",
      "\n",
      "Target:\n",
      "You are using your own `resolveToArgumentMatch` function here\r\n",
      "\r\n",
      "Please, replace it with `org.jetbrains.kotlin.resolve.calls.callUtil.getParameterForArgument`, it is designed specifically for this case\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "I think we can remove this annotation now.\n",
      "\n",
      "Target:\n",
      "You can use `@SinceKotlin(\"1.3\")` + `@ExperimentalStdlibApi`. This way new functions could be published in 1.3.x release\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "I think we should just remove this annotation.\n",
      "\n",
      "Target:\n",
      "For JS_IR we copy JS sources during build, so this suppress should not be needed.\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "This is the only change in this file.\n",
      "\n",
      "Target:\n",
      "I would prefer to keep original separation: visitWhileLoop/visitDoWhileLoop\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "I think we should use the `KtPsiFactory` here instead of `Editor`\n",
      "\n",
      "Target:\n",
      "Better add \"lambda\" (e.g,  \"Convert to multi-line lambda\") to the end of the title as it's not quite obvious for the user what should that intension do in case when many intentions are suggested to the user\n",
      "==========================================================================================================\n",
      "Generated:\n",
      "I think we should also check for `resultingDescriptor.isNull` here.\n",
      "\n",
      "Target:\n",
      "But the called function may have an arbitrary name, see https://youtrack.jetbrains.com/issue/KT-33741#focus=streamItem-27-3697460.0-0\n"
     ]
    }
   ],
   "source": [
    "for pred_and_target in predictions_and_targets[:15]:\n",
    "    print(\"==========================================================================================================\")\n",
    "    print(f\"Generated:\\n{pred_and_target['pred'][5:]}\\n\\nTarget:\\n{pred_and_target['target']}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T23:00:22.499607Z",
     "start_time": "2023-09-16T23:00:22.496355Z"
    }
   },
   "id": "c6f4788c671cd53"
  },
  {
   "cell_type": "markdown",
   "source": [
    "With Kotlin there is more general comments like those, which were present before adding special tokens. For example, \"This is a bit of a hack...\".\n",
    "These tokens were improving the models \"understanding\" of the code. I would assume that the model understands Kotlin less. \n",
    "\n",
    "However, there are some comments that hit the target: the \"unused import\" one and the \"remove annotation\" one. With both imports and annotations looking like in Java, it's sensible that the model was able to write this. \n",
    "\n",
    "Although it's worth mentioning, that in another diff with annotations, the model also proposed to remove it, but it wasn't the ground truth answer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9975e8d5f0ad3f0b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### With eyes and with numbers\n",
    "\n",
    "Ok, looking with a naked eye, it seems like the model understands Kotlin less than other languages. (Judging by the number of general comments)\n",
    "\n",
    "But let's try to estimate the \"goodness\" of the predictions with some metric. The metric that is used in the paper (and, therefore, we know the score of the model on the official dataset) is BLEU. \n",
    "\n",
    "Even though, as they explain in the paper, this is not a good metric, we don't have a better metric apart from human expert estimation, because answers containing entirely different tokens could be right.\n",
    "\n",
    "For human expert estimation I don't have spare humans to spend. So let's calculate the metric!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "478b6581931283b6"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.009101684249318965, 'precisions': [0.18890882146478433, 0.024603698811096433, 0.009734280452512496, 0.005610622779128483], 'brevity_penalty': 0.4054897436135259, 'length_ratio': 0.5255800606706568, 'translation_length': 12821, 'reference_length': 24394}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "with open(\"./dataset/github/jetbrains_kotlin_preds.jsonl\", \"r\") as file:\n",
    "    pred_targets = [json.loads(line) for line in file.readlines()]\n",
    "    predictions = [pred_target['pred'] for pred_target in pred_targets]\n",
    "    references = [[pred_target['target']] for pred_target in pred_targets]\n",
    "\n",
    "print(bleu.compute(predictions=predictions, references=references))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T23:30:08.353642Z",
     "start_time": "2023-09-16T23:30:05.987999Z"
    }
   },
   "id": "4dac364fb6a95ea4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ok, going back to the paper, they have BLEU values of 7.97, for example. This is strange, because BLEU calculates the ratio and the values should be in [0, 1].\n",
    "\n",
    "Even if we assume that they scaled it to [0, 100], it doesn't match with the following calculation:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "823eed66c2f7313e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from dataset.KotlinDataset import KotlinDataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T01:02:09.714583Z",
     "start_time": "2023-09-17T01:02:09.680729Z"
    }
   },
   "id": "a89122de653e115f"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.17647058823529413, 0.0, 0.0, 0.0], 'brevity_penalty': 0.7026185226629954, 'length_ratio': 0.7391304347826086, 'translation_length': 17, 'reference_length': 23}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codereviewer\")\n",
    "\n",
    "preds = [\" \".join(tokenizer.tokenize(\"This is an important debugging help and shouldn't be lower than the default visible INFO.\")).lower()]\n",
    "refs = [[\" \".join(tokenizer.tokenize(\"This change prevents a user understanding how their server is configured. Best to leave at `info` level.\")).lower()]]\n",
    "\n",
    "print(bleu.compute(predictions=preds, references=refs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T23:53:08.771498Z",
     "start_time": "2023-09-16T23:53:07.452245Z"
    }
   },
   "id": "7c91f723f0cc80da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The BLEU score is claimed to be `7.97` here, but it's `0` (at least by definition from HuggingFace). I tried computing it with their tokenizer as well, and it was 0 as well... \n",
    "\n",
    "Alright, let's instead try to improve the metric with fine-tuning the model on our Kotlin mini-dataset.\n",
    "\n",
    "I've already divided the data into train and validation/test using `github_divide.py` script that I wrote. 80% of the comments went to train and 20% on validation and testing. We don't distinguish validation and testing, because the dataset is small. \n",
    "\n",
    "Also, worth mentioning that our mini-dataset doesn't contain examples with code that shouldn't be commented. Let's say our model will look only at the code that should be commented for the sake of keeping the dataset small (so that I can run this toy fine-tuning everything relatively fast)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8854273ef14139f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kholkinilia/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/2 : < :, Epoch 0.50/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codereviewer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/codereviewer\")\n",
    "\n",
    "train_dataset = KotlinDataset(tokenizer, \"./dataset/fine_tuning/train.jsonl\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    save_steps=5,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    do_train=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./fine_tuned_model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T01:41:57.575669Z",
     "start_time": "2023-09-17T01:41:33.247260Z"
    }
   },
   "id": "9ef4b6c64f8ad4d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
